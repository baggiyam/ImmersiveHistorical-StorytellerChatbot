{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f94596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c91d4a",
   "metadata": {},
   "source": [
    "# Transcript Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77054aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessed 28 files into 1437 chunks\n",
      "üìÅ Saved to pinecone_data.json\n"
     ]
    }
   ],
   "source": [
    "def preprocess_transcripts(transcripts_dir=\"transcripts\", output_json=\"pinecone_data.json\"):\n",
    "    transcripts_path = Path(transcripts_dir)\n",
    "    if not transcripts_path.exists():\n",
    "        raise FileNotFoundError(f\"Transcript folder not found: {transcripts_path}\")\n",
    "    \n",
    "    transcript_files = list(transcripts_path.glob(\"*.txt\"))\n",
    "    if not transcript_files:\n",
    "        raise FileNotFoundError(f\"No .txt files found in {transcripts_path}\")\n",
    "\n",
    "    # Map video IDs to topics\n",
    "    topic_map = {\n",
    "        \"4jEad6zxaFk\": \"The Great Pyramids\",\n",
    "        \"BR2ZMj3o5EU\": \"The Great Pyramids\",\n",
    "        \"9yD9GxzKd_Q\": \"The Great Pyramids\",\n",
    "        \"vJucA4FOTSI\": \"The Great Pyramids\",\n",
    "        \"25sBBCPeRvY\": \"The Great Pyramids\",\n",
    "        \"k3QiW0gEpYM\": \"The Great Pyramids\",\n",
    "        \"Fo8issWL-tI\": \"The Great Pyramids\",\n",
    "        \"Us2v5O5EkZM\": \"Roman Forum\",\n",
    "        \"OWHkpLVskk\": \"Roman Forum\",\n",
    "        \"k4P5W1DKTBI\": \"Roman Forum\",\n",
    "        \"evmyQGmuzqA\": \"Roman Forum\",\n",
    "        \"MGd3BVW3vYs\": \"Roman Forum\",\n",
    "        \"CVyuqIoB7qg\": \"Roman Forum\",\n",
    "        \"zxKPjD8urG4\": \"Roman Forum\",\n",
    "        \"ClUdLS-UAZ0\": \"Ancient Greece\",\n",
    "        \"Y_B0bh7MXgI\": \"Ancient Greece\",\n",
    "        \"OmH4FDs3yl0\": \"Ancient Greece\",\n",
    "        \"sFU-rJXQlxI\": \"Ancient Greece\",\n",
    "        \"Mk-OyRI7c7Q\": \"Ancient Greece\",\n",
    "        \"4M-4M4LyUB0\": \"Ancient Greece\",\n",
    "        \"VB_WUMtdjpU\": \"Machu Picchu\",\n",
    "        \"JMAKRKkdOlw\": \"Machu Picchu\",\n",
    "        \"Z94cpOLx_JQ\": \"Machu Picchu\",\n",
    "        \"ZR_mEpS4Tvw\": \"Machu Picchu\",\n",
    "        \"naEFqvUiU0c\": \"Machu Picchu\",\n",
    "        \"Q-mAWItV2q0\": \"Mesopotamia\",\n",
    "        \"fBOD64ow5eo\": \"Mesopotamia\",\n",
    "        \"9q7r0XZUyAk\": \"Sangam Tamil\",\n",
    "        \"GgalhomEIcc\": \"Sangam Tamil\",\n",
    "    }\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for file_path in transcript_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # Extract video ID from filename (assuming it's before first underscore)\n",
    "        video_id = file_path.stem.split('_')[0]\n",
    "        video_title = file_path.stem.replace('_transcript', '').replace('_', ' ')\n",
    "        topic = topic_map.get(video_id, \"Unknown\")\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": file_path.name,\n",
    "                \"video_title\": video_title,\n",
    "                \"file_path\": str(file_path),\n",
    "                \"topic\": topic\n",
    "            }\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_id\": f\"{file_path.stem}_chunk_{i}\",\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks)\n",
    "            })\n",
    "\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    pinecone_records = [{\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"text\": chunk.page_content,\n",
    "        \"metadata\": chunk.metadata\n",
    "    } for chunk in all_chunks]\n",
    "\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(pinecone_records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Preprocessed {len(transcript_files)} files into {len(all_chunks)} chunks\")\n",
    "    print(f\"üìÅ Saved to {output_json}\")\n",
    "                  \n",
    "            \n",
    "\n",
    "\n",
    "# Run the full pipeline\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b876791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
